# Data Quality Assessment Tool

The scripts included in this repo can be used as a way to audit the quality of a dataset through 6 dimensions. Currently the focus is on data generated by IoT sensors, specifically used in the context of assessing various aspects of smart cities, such as AQM, ITMS, etc.

For each of the dimensions that are used to quantify the quality of a dataset, the tool aims to provide a metric score between 0 and 1, where 1 is the highest possible score, indicating a 100% score.
Currently, the tool is able to assess five of these dimensions, namely:

- Format Adherence
- Format Completeness
- Additional Attributes Present
- Inter-Arrival Timeliness
- Duplicate Presence

A note to remember is that each dataset has a linked JSON Schema that defines the attributes that can be present in the dataset, whether there are any required attributes, and what units and datatypes these attributes need to have.

### Format Adherence Metric

This metric serves to compare the dataset with the JSON Schema that it is linked to, and checks each attribute to see if the format present in the dataset matches the format specified in the Schema. The output is a score between 0 and 1, where 1 is the highest possible score, indicating that the dataset is 100% adherent to the format.

### Format Completeness

This metric serves to compare the dataset with the JSON Schema that it is linked to and checks whether all the attributes marked as 'required' in the schema are present in the dataset. The output is a score between 0 and 1, where 1 is the highest possible score, indicating that the dataset is 100% adherent to the required attributes list.

### Additional Attributes Present

This metric serves to compare the dataset with the JSON Schema that it is linked to and checks whether the Schema limits the presence of additional attributes and whether there are any additional attributes not mentioned in the Schema present in the dataset. The output is a score between 0 and 1, where 1 is the highest possible score, indicating that there are no additional attributes present.

### Duplicate Presence

This metric serves to check two columns that are input by the user for any duplicate values in the dataset. A value is considered to be a duplicate if both columns contain the exact same values for any data packet. Here, it is important to choose the appropriate combination of columns to correctly find values that are duplicates. This step is also crucial for some other operations such as Inter-Arrival Timeliness. the output is a score between 0 and 1, where 1 is the highest possible score, indicating the percentage of duplicates that were found in the dataset when compared to the total number of data packets.

### Inter-Arrival Timeliness

This metric calculates the delta between the data packet timestamps and plots a histogram of these time deltas for all sensors in a particular time period. The output is a plot in '.pdf' format as well as a score from 0 to 1, where 1 is the highest possible score, indicating that there are no data packets that are sending data with a time delta of lesser than or greater than the (*mode +/- alpha x mode). The script also outputs the mean, standard deviation, and the mode of the time deltas of any dataset.

## How to Run the Tool

### Required libraries and packages
Once in the scripts folder, run the following command to install the package and library dependencies:

```console
pip install -r requirements.txt
```

Prior to running the tool, ensure that the IUDX SDK is installed on your computer using the following command.

```console
pip install git+https://github.com/datakaveri/iudx-python-sdk
```

### Running the tool
Clone the repo from:

``` console
https://github.com/novoneel-iudx/data-quality-assessment.git
```
Once inside the directory where the repo was cloned, run:
```console
pip install .
```

Present in the Config folder is a config.json with the name of the dataset prepended to it to aid in identification. This file requires you to input the name of the datafile as well as select the attributes that you would like to check for duplicates. In this case, the name of the datafile and the appropriate attributes for selection are already included in the file as below: 
- *observationDateTime*
- *id* for AQM data & *trip_id* for ITMS data

Present in the *data* folder in the repository is a sample dataset of ITMS data from Surat, as well as a sample of AQM data from Pune. Inside the Schemas folder are the corresponding schemas for these datasets. In order to assess the quality of these datasets, the scripts can be run in the order below with included system arguments:

```console
python3 DuplicationDetection.py <../config/config.json>
python3 InterArrivalTime.py <../config/config.json>
python3 FormatValidation.py <../config/config.json>
```

Ensure that the datasets are in *.csv* format and are located in the *data* folder.

The output report file will be generated in a *.json* format and and a *.pdf* format and will be located in the outputReports folder. All the scripts will append their results to the same output file.
