# Data Quality Assessment Tool

The scripts included in this repo can be used as a way to audit the quality of a dataset through 6 dimensions. Currently the focus is on data generated by IoT sensors, specifically used in the context of assessing various aspects of smart cities, such as AQM, ITMS, etc.

For each of the dimensions that are used to quantify the quality of a dataset, the tool aims to provide a metric score between 0 and 1, where 1 is the highest possible score, indicating a 100% score.
Currently, the tool is able to assess five of these dimensions, namely:

- Format Adherence
- Format Completeness
- Additional Attributes Present
- Inter-Arrival Timeliness
- Duplicate Presence

A note to remember is that each dataset has a linked JSON Schema that defines the attributes that can be present in the dataset, whether there are any required attributes, and what units and datatypes these attributes need to have.

### Format Adherence Metric

This metric serves to compare the dataset with the JSON Schema that it is linked to, and checks each attribute to see if the format present in the dataset matches the format specified in the Schema. The output is a score between 0 and 1, where 1 is the highest possible score, indicating that the dataset is 100% adherent to the format.

### Format Completeness

This metric serves to compare the dataset with the JSON Schema that it is linked to and checks whether all the attributes marked as 'required' in the schema are present in the dataset. The output is a score between 0 and 1, where 1 is the highest possible score, indicating that the dataset is 100% adherent to the required attributes list.

### Additional Attributes Present

This metric serves to compare the dataset with the JSON Schema that it is linked to and checks whether the Schema limits the presence of additional attributes and whether there are any additional attributes not mentioned in the Schema present in the dataset. The output is a score between 0 and 1, where 1 is the highest possible score, indicating that there are no additional attributes present.

### Duplicate Presence

This metric serves to check two columns that are input by the user for any duplicate values in the dataset. A value is considered to be a duplicate if both columns contain the exact same values for any data packet. Here, it is important to choose the appropriate combination of columns to correctly find values that are duplicates. This step is also crucial for some other operations such as Inter-Arrival Timeliness. the output is a score between 0 and 1, where 1 is the highest possible score, indicating the percentage of duplicates that were found in the dataset when compared to the total number of data packets.

### Inter-Arrival Timeliness

This metric calculates the delta between the data packet timestamps and plots a histogram of these time deltas for all sensors in a particular time period. The output is a plot in '.pdf' format as well as a score from 0 to 1, where 1 is the highest possible score, indicating that there are no data packets that are sending data with a time delta of lesser than or greater than the (*mode +/- alpha x mode). The script also outputs the mean, standard deviation, and the mode of the time deltas of any dataset.

## How to Run the Tool

### Required libraries and packages

Ensure that you have the below packages installed:
- pandas
- numpy
- matplotlib.pyplot
- re
- json
- jsonschema
- os
- sys
- Path
- scipy
- math
- requests
- re
- logging

Prior to running the tool, ensure that the IUDX SDK is installed on your computer using the following command.

```console
pip install git+https://github.com/datakaveri/iudx-python-sdk
```

The tool can be run by downloading the repository to a local system with python3 installed and unpacking the folder structure present in the repository (a python virtual environment is highly recommended for this operation).

Present in the Config folder is a config.json file that requires you to input the name of the data file as well as select the attributes that you would like to check for duplicates. A recommended selection are the following columns: 
- *observationDateTime*
- *id* for AQM data & *trip_id* for ITMS data

Present in the *data* folder in the repository is a sample dataset of ITMS data from Surat, as well as a couple of others.. Inside the Schemas folder are the corresponding schemas for these datasets. In order to assess the quality of these datasets, the scripts can be run in the order below with included system arguments:

```console
python3 FormatValidation.py <test_data> <test_schema>
python3 DuplicationDetection.py <../config/config.json>
python3 InterArrivalTime.py <../config/config.json>
```

Ensure that the datasets in *.csv* format are located in the *data* folder.

The output report file will be generated in a *.json* format and and a *.pdf* format and will be located in the Reports folder. All the scripts will append their results to the same output file.
